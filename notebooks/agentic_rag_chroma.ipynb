{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGCHAIN_TRACING_V2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "LANGCHAIN_ENDPOINT = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GEMINI_API_KEY)\n",
    "url = [\n",
    "    \"https://langchain-ai.github.io/langgraph/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/', 'title': 'Home', 'description': 'Build language agents as graphs', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\nJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n              Home\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n    \\n  \\n  Home\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  API reference\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Home\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n  \\n    Home\\n  \\n\\n          \\n\\n\\n\\n\\n\\n    \\n  \\n    Get started\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    Get started\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    Learn the basics\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Deployment\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Guides\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    Guides\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    How-to Guides\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Concepts\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Tutorials\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Resources\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    Resources\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    Prebuilt Agents\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Companies using LangGraph\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    FAQ\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Troubleshooting\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph Academy Course\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    API reference\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      \\n        Why use LangGraph?\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        LangGraph’s ecosystem\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        Pairing with LangGraph Platform\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        Additional resources\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        Acknowledgements\\n      \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNote\\nLooking for the JS version? See the JS repo and the JS docs.\\n\\nLangGraph — used by Replit, Uber, LinkedIn, GitLab and more — is a low-level orchestration framework for building controllable agents. While langchain provides integrations and composable components to streamline LLM application development, the LangGraph library enables agent orchestration — offering customizable architectures, long-term memory, and human-in-the-loop to reliably handle complex tasks.\\npip install -U langgraph\\n\\nTo learn more about how to use LangGraph, check out the docs. We show a simple example below of how to create a ReAct agent.\\n# This code depends on pip install langchain[anthropic]\\nfrom langgraph.prebuilt import create_react_agent\\n\\ndef search(query: str):\\n    \"\"\"Call to surf the web.\"\"\"\\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\\n        return \"It\\'s 60 degrees and foggy.\"\\n    return \"It\\'s 90 degrees and sunny.\"\\n\\nagent = create_react_agent(\"anthropic:claude-3-7-sonnet-latest\", tools=[search])\\nagent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\\n)\\n\\nWhy use LangGraph?¶\\nLangGraph is built for developers who want to build powerful, adaptable AI agents. Developers choose LangGraph for:\\n\\nReliability and controllability. Steer agent actions with moderation checks and human-in-the-loop approvals. LangGraph persists context for long-running workflows, keeping your agents on course.\\nLow-level and extensible. Build custom agents with fully descriptive, low-level primitives – free from rigid abstractions that limit customization. Design scalable multi-agent systems, with each agent serving a specific role tailored to your use case.\\nFirst-class streaming support. With token-by-token streaming and streaming of intermediate steps, LangGraph gives users clear visibility into agent reasoning and actions as they unfold in real time.\\n\\nLangGraph is trusted in production and powering agents for companies like:\\n\\nKlarna: Customer support bot for 85 million active users\\nElastic: Security AI assistant for threat detection\\nUber: Automated unit test generation\\nReplit: Code generation\\nAnd many more (see list here)\\n\\nLangGraph’s ecosystem¶\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\n\\nLangSmith — Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\\nLangGraph Platform — Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in LangGraph Studio.\\n\\nPairing with LangGraph Platform¶\\nWhile LangGraph is our open-source agent orchestration framework, enterprises that need scalable agent deployment can benefit from LangGraph Platform.\\nLangGraph Platform can help engineering teams:\\n\\nAccelerate agent development: Quickly create agent UXs with configurable templates and LangGraph Studio for visualizing and debugging agent interactions.\\nDeploy seamlessly: We handle the complexity of deploying your agent. LangGraph Platform includes robust APIs for memory, threads, and cron jobs plus auto-scaling task queues & servers.\\nCentralize agent management & reusability: Discover, reuse, and manage agents across the organization. Business users can also modify agents without coding.\\n\\nAdditional resources¶\\n\\nLangChain Academy: Learn the basics of LangGraph in our free, structured course.\\nTutorials: Simple walkthroughs with guided examples on getting started with LangGraph.\\nTemplates: Pre-built reference apps for common agentic workflows (e.g. ReAct agent, memory, retrieval etc.) that can be cloned and adapted.\\nHow-to Guides: Quick, actionable code snippets for topics such as streaming, adding memory & persistence, and design patterns (e.g. branching, subgraphs, etc.).\\nAPI Reference: Detailed reference on core classes, methods, how to use the graph and checkpointing APIs, and higher-level prebuilt components.\\nBuilt with LangGraph: Hear how industry leaders use LangGraph to ship powerful, production-ready AI applications.\\n\\nAcknowledgements¶\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\n\\n\\n        Was this page helpful?\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Learn the basics\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs Insiders\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCookie consent\\nWe use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they\\'re searching for. Clicking \"Accept\" makes our documentation better. Thank you! ❤️\\n\\n\\n\\n\\n\\n\\n\\n          Google Analytics\\n        \\n\\n\\n\\n\\n\\n          GitHub\\n        \\n\\n\\n\\n\\nAccept\\nReject\\n\\n\\n\\n\\n\\n\\n\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "docs = [WebBaseLoader(url).load()]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "print(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(max_results=2)\n",
    "# tools = [tool]\n",
    "# tool.invoke(\"What's a 'node' in LangGraph?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Prompt[rlm/rag-prompt]********************\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: \u001b[33;1m\u001b[1;3m{question}\u001b[0m \n",
      "Context: \u001b[33;1m\u001b[1;3m{context}\u001b[0m \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Generate Response Node\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm =  ChatGroq(temperature=0, model_name=\"gemma2-9b-it\", groq_api_key=GROQ_API_KEY)\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "print(\"*\" * 20 + \"Prompt[rlm/rag-prompt]\" + \"*\" * 20)\n",
    "prompt = hub.pull(\"rlm/rag-prompt\",api_key= LANGSMITH_API_KEY).pretty_print()  # Show what the prompt looks like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web search Agent\n",
    "\n",
    "def web_search_agent(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"messages\"]\n",
    "    query = question[0].content\n",
    "    print(question)\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": query})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    # web = Document(page_content=web_results)\n",
    "\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [web_results]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reframe the question\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n\n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n\n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question}\n",
    "    \\n ------- \\n\n",
    "    Formulate and return an improved question. Only return the improved question in one line and nothing else\"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    llm =  ChatGroq(temperature=0, model_name=\"gemma2-9b-it\", groq_api_key=GROQ_API_KEY)\n",
    "    response = llm.invoke(msg)\n",
    "    return {\"messages\": [response.content]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name = \"AI-info-retriever\",\n",
    "    description= \"Search and return information about langgraph\",\n",
    ")\n",
    "\n",
    "\n",
    "tools = [retriever_tool]\n",
    "\n",
    "# retrieval agent\n",
    "def retrieve_agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    print(messages)\n",
    "    model =  ChatGroq(temperature=0, model_name=\"gemma2-9b-it\", groq_api_key=GROQ_API_KEY)\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document grader\n",
    "def grade_documents(state) -> Literal[\"generate_response\", \"web_search\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model =  ChatGroq(temperature=0, model_name=\"gemma2-9b-it\", groq_api_key=GROQ_API_KEY)\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate_response\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"web_search\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define graph “workflow” add nodes and edges/relationships and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict,Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START,END,StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "\n",
    "# Nodes\n",
    "workflow.add_node(\"web_search\",web_search_agent)\n",
    "workflow.add_node(\"rewrite\",rewrite)\n",
    "workflow.add_node(\"retrieve\",retrieve_agent)\n",
    "retriever_node = ToolNode([retriever_tool])  # create retriever tool node\n",
    "workflow.add_node(\"retriever\", retriever_node)\n",
    "workflow.add_node(\"generate_response\", generate)\n",
    "\n",
    "# Edges\n",
    "# workflow.add_edge(START,\"retrieve\")\n",
    "workflow.add_edge(START,\"rewrite\")\n",
    "workflow.add_edge(\"rewrite\",\"retrieve\")\n",
    "workflow.add_edge(\"web_search\",\"generate_response\")\n",
    "workflow.add_conditional_edges(\"retrieve\",\n",
    "                               tools_condition,\n",
    "                               {\"tools\":\"retriever\"},\n",
    "                               END)\n",
    "workflow.add_conditional_edges(\"retriever\",grade_documents)\n",
    "workflow.add_edge(\"generate_response\",END)\n",
    "\n",
    "# Compile workflow\n",
    "graph = workflow.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---TRANSFORM QUERY---\n",
      "\"Assistant 'rewrite':\"\n",
      "{'messages': ['Can you explain the rules and gameplay of cricket?  \\n']}\n",
      "---CALL AGENT---\n",
      "[HumanMessage(content='how do you play cricket?', additional_kwargs={}, response_metadata={}, id='afebdbdf-8e6a-42af-932c-97dd11d01160'), HumanMessage(content='Can you explain the rules and gameplay of cricket?  \\n', additional_kwargs={}, response_metadata={}, id='415f217b-3c67-4ccf-96af-9afa3d5b46e0')]\n",
      "\"Assistant 'retrieve':\"\n",
      "{ 'messages': [ AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_4twm', 'function': {'arguments': '{\"query\":\"Explain the rules and gameplay of cricket\"}', 'name': 'AI-info-retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 984, 'total_tokens': 1075, 'completion_time': 0.165454545, 'prompt_time': 0.035698518, 'queue_time': 0.23357048999999996, 'total_time': 0.201153063}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-72386ee1-7a8b-468f-9888-62c3d754a2a5-0', tool_calls=[{'name': 'AI-info-retriever', 'args': {'query': 'Explain the rules and gameplay of cricket'}, 'id': 'call_4twm', 'type': 'tool_call'}], usage_metadata={'input_tokens': 984, 'output_tokens': 91, 'total_tokens': 1075})]}\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "\"Assistant 'retriever':\"\n",
      "{ 'messages': [ ToolMessage(content='Thanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n                Next\\n\\nThanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n                Next\\n\\nThanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n                Next\\n\\nThanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n                Next', name='AI-info-retriever', id='bd270853-ab41-4933-a889-4dfba3e212a9', tool_call_id='call_4twm')]}\n",
      "---WEB SEARCH---\n",
      "[HumanMessage(content='how do you play cricket?', additional_kwargs={}, response_metadata={}, id='afebdbdf-8e6a-42af-932c-97dd11d01160'), HumanMessage(content='Can you explain the rules and gameplay of cricket?  \\n', additional_kwargs={}, response_metadata={}, id='415f217b-3c67-4ccf-96af-9afa3d5b46e0'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_4twm', 'function': {'arguments': '{\"query\":\"Explain the rules and gameplay of cricket\"}', 'name': 'AI-info-retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 984, 'total_tokens': 1075, 'completion_time': 0.165454545, 'prompt_time': 0.035698518, 'queue_time': 0.23357048999999996, 'total_time': 0.201153063}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-72386ee1-7a8b-468f-9888-62c3d754a2a5-0', tool_calls=[{'name': 'AI-info-retriever', 'args': {'query': 'Explain the rules and gameplay of cricket'}, 'id': 'call_4twm', 'type': 'tool_call'}], usage_metadata={'input_tokens': 984, 'output_tokens': 91, 'total_tokens': 1075}), ToolMessage(content='Thanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n                Next\\n\\nThanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n                Next\\n\\nThanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n                Next\\n\\nThanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n                Next', name='AI-info-retriever', id='bd270853-ab41-4933-a889-4dfba3e212a9', tool_call_id='call_4twm')]\n",
      "\"Assistant 'web_search':\"\n",
      "{ 'messages': [ 'As in most field games, the goal of cricket is to score '\n",
      "                'points, called runs, against the opposing team by running '\n",
      "                'from one point to another before the play can be ended or '\n",
      "                \"you're run out by the defenders, who are called the “fielding \"\n",
      "                'team.” The team at bat is called the “batting team.”[2] X '\n",
      "                'Research source The batsmen try to hit the ball after it is '\n",
      "                'bowled by the bowler for the fielding team, and then switch '\n",
      "                'positions without getting an out to score runs.[3] X Research '\n",
      "                'source\\n'\n",
      "                \"Cricket is the world's second-most popular sport, but perhaps \"\n",
      "                \"remains the most confusing. The game's rules, shape of the \"\n",
      "                'pitch and the']}\n",
      "---GENERATE---\n",
      "\"Assistant 'generate_response':\"\n",
      "{ 'messages': [ 'The batting team tries to score runs by hitting the ball '\n",
      "                \"bowled by the fielding team's bowler.  They run between two \"\n",
      "                'points on the field to score, aiming to avoid getting \"out\".  '\n",
      "                'The fielding team tries to prevent runs and get the batsmen '\n",
      "                'out. \\n']}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"how do you play cricket?\"),\n",
    "    ]\n",
    "}\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Assistant '{key}':\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---TRANSFORM QUERY---\n",
      "\"Assistant 'rewrite':\"\n",
      "{'messages': ['What tools are available within the Langgraph platform?  \\n']}\n",
      "---CALL AGENT---\n",
      "[HumanMessage(content='what are tools in langgraph', additional_kwargs={}, response_metadata={}, id='f24b8b81-c33c-4186-a5e8-5e06c28e59ca'), HumanMessage(content='What tools are available within the Langgraph platform?  \\n', additional_kwargs={}, response_metadata={}, id='aa61fa23-c629-407d-947f-72cc9576a65d')]\n",
      "\"Assistant 'retrieve':\"\n",
      "{ 'messages': [ AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_298z', 'function': {'arguments': '{\"query\":\"What tools are available within the Langgraph platform?\"}', 'name': 'AI-info-retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 984, 'total_tokens': 1077, 'completion_time': 0.169090909, 'prompt_time': 0.034229621, 'queue_time': 0.236113945, 'total_time': 0.20332053}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-47e7923b-72eb-406b-8d48-1ce45d7cfbd9-0', tool_calls=[{'name': 'AI-info-retriever', 'args': {'query': 'What tools are available within the Langgraph platform?'}, 'id': 'call_298z', 'type': 'tool_call'}], usage_metadata={'input_tokens': 984, 'output_tokens': 93, 'total_tokens': 1077})]}\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "\"Assistant 'retriever':\"\n",
      "{ 'messages': [ ToolMessage(content='Standardizing these components allows individuals and teams to focus on the behavior\\nof their agent, instead of its supporting infrastructure.\\nThrough LangGraph Platform, LangGraph also provides tooling for\\nthe development, deployment, debugging, and monitoring of your applications.\\nLangGraph integrates seamlessly with\\nLangChain and\\nLangSmith (but does not require them).\\nTo learn more about LangGraph, check out our first LangChain Academy\\ncourse, Introduction to LangGraph, available for free\\nhere.\\nLangGraph Platform¶\\nLangGraph Platform is infrastructure for deploying LangGraph agents. It is a commercial solution for deploying agentic applications to production, built on the open-source LangGraph framework. The LangGraph Platform consists of several components that work together to support the development, deployment, debugging, and monitoring of LangGraph applications: LangGraph Server (APIs), LangGraph SDKs (clients for the APIs), LangGraph CLI (command line tool for building the server), and LangGraph Studio (UI/debugger).\\nSee deployment options here\\n(includes a free tier).\\nHere are some common issues that arise in complex deployments, which LangGraph Platform addresses:\\n\\nStandardizing these components allows individuals and teams to focus on the behavior\\nof their agent, instead of its supporting infrastructure.\\nThrough LangGraph Platform, LangGraph also provides tooling for\\nthe development, deployment, debugging, and monitoring of your applications.\\nLangGraph integrates seamlessly with\\nLangChain and\\nLangSmith (but does not require them).\\nTo learn more about LangGraph, check out our first LangChain Academy\\ncourse, Introduction to LangGraph, available for free\\nhere.\\nLangGraph Platform¶\\nLangGraph Platform is infrastructure for deploying LangGraph agents. It is a commercial solution for deploying agentic applications to production, built on the open-source LangGraph framework. The LangGraph Platform consists of several components that work together to support the development, deployment, debugging, and monitoring of LangGraph applications: LangGraph Server (APIs), LangGraph SDKs (clients for the APIs), LangGraph CLI (command line tool for building the server), and LangGraph Studio (UI/debugger).\\nSee deployment options here\\n(includes a free tier).\\nHere are some common issues that arise in complex deployments, which LangGraph Platform addresses:\\n\\nStandardizing these components allows individuals and teams to focus on the behavior\\nof their agent, instead of its supporting infrastructure.\\nThrough LangGraph Platform, LangGraph also provides tooling for\\nthe development, deployment, debugging, and monitoring of your applications.\\nLangGraph integrates seamlessly with\\nLangChain and\\nLangSmith (but does not require them).\\nTo learn more about LangGraph, check out our first LangChain Academy\\ncourse, Introduction to LangGraph, available for free\\nhere.\\nLangGraph Platform¶\\nLangGraph Platform is infrastructure for deploying LangGraph agents. It is a commercial solution for deploying agentic applications to production, built on the open-source LangGraph framework. The LangGraph Platform consists of several components that work together to support the development, deployment, debugging, and monitoring of LangGraph applications: LangGraph Server (APIs), LangGraph SDKs (clients for the APIs), LangGraph CLI (command line tool for building the server), and LangGraph Studio (UI/debugger).\\nSee deployment options here\\n(includes a free tier).\\nHere are some common issues that arise in complex deployments, which LangGraph Platform addresses:\\n\\nStandardizing these components allows individuals and teams to focus on the behavior\\nof their agent, instead of its supporting infrastructure.\\nThrough LangGraph Platform, LangGraph also provides tooling for\\nthe development, deployment, debugging, and monitoring of your applications.\\nLangGraph integrates seamlessly with\\nLangChain and\\nLangSmith (but does not require them).\\nTo learn more about LangGraph, check out our first LangChain Academy\\ncourse, Introduction to LangGraph, available for free\\nhere.\\nLangGraph Platform¶\\nLangGraph Platform is infrastructure for deploying LangGraph agents. It is a commercial solution for deploying agentic applications to production, built on the open-source LangGraph framework. The LangGraph Platform consists of several components that work together to support the development, deployment, debugging, and monitoring of LangGraph applications: LangGraph Server (APIs), LangGraph SDKs (clients for the APIs), LangGraph CLI (command line tool for building the server), and LangGraph Studio (UI/debugger).\\nSee deployment options here\\n(includes a free tier).\\nHere are some common issues that arise in complex deployments, which LangGraph Platform addresses:', name='AI-info-retriever', id='e8d4e966-1e24-477c-a188-18e683270aa7', tool_call_id='call_298z')]}\n",
      "---GENERATE---\n",
      "\"Assistant 'generate_response':\"\n",
      "{ 'messages': [ 'LangGraph provides tooling for development, deployment, '\n",
      "                'debugging, and monitoring applications.  These tools include '\n",
      "                'LangGraph Server, LangGraph SDKs, LangGraph CLI, and '\n",
      "                'LangGraph Studio.  LangGraph integrates with LangChain and '\n",
      "                'LangSmith but is not dependent on them. \\n'\n",
      "                '\\n'\n",
      "                '\\n']}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"what are tools in langgraph\"),\n",
    "    ]\n",
    "}\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Assistant '{key}':\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lang_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
