{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGCHAIN_TRACING_V2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "LANGCHAIN_ENDPOINT = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GEMINI_API_KEY)\n",
    "url = [\n",
    "    \"https://langchain-ai.github.io/langgraph/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/', 'title': 'Home', 'description': 'Build language agents as graphs', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\nJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n              Home\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n    \\n  \\n  Home\\n\\n        \\n\\n\\n\\n          \\n  \\n    \\n  \\n  API reference\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Home\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n  \\n    Home\\n  \\n\\n          \\n\\n\\n\\n\\n\\n    \\n  \\n    Get started\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    Get started\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    Learn the basics\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Deployment\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Guides\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    Guides\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    How-to Guides\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Concepts\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Tutorials\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    Resources\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n            \\n  \\n    Resources\\n  \\n\\n          \\n\\n\\n\\n\\n    \\n  \\n    Prebuilt Agents\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Companies using LangGraph\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    FAQ\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n    \\n  \\n    Troubleshooting\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    \\n  \\n    LangGraph Academy Course\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n  \\n    API reference\\n  \\n\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      \\n        Overview\\n      \\n    \\n\\n\\n\\n\\n\\n\\n      \\n        Why use LangGraph?\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        LangGraph Platform\\n      \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      \\n        Installation\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        Example\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        Documentation\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        Resources\\n      \\n    \\n\\n\\n\\n\\n\\n      \\n        Contributing\\n      \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nü¶úüï∏Ô∏èLangGraph¬∂\\n\\n\\n\\n\\n‚ö° Building language agents as graphs ‚ö°\\n\\nNote\\nLooking for the JS version? See the JS repo and the JS docs.\\n\\nOverview¬∂\\nLangGraph is a library for building\\nstateful, multi-actor applications with LLMs, used to create agent and multi-agent\\nworkflows. Check out an introductory tutorial here.\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\nWhy use LangGraph?¬∂\\nLangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more. LangGraph provides fine-grained control over both the flow and state of your agent applications. It implements a central persistence layer, enabling features that are common to most agent architectures:\\n\\nMemory: LangGraph persists arbitrary aspects of your application\\'s state,\\nsupporting memory of conversations and other updates within and across user\\ninteractions;\\nHuman-in-the-loop: Because state is checkpointed, execution can be interrupted\\nand resumed, allowing for decisions, validation, and corrections at key stages via\\nhuman input.\\n\\nStandardizing these components allows individuals and teams to focus on the behavior\\nof their agent, instead of its supporting infrastructure.\\nThrough LangGraph Platform, LangGraph also provides tooling for\\nthe development, deployment, debugging, and monitoring of your applications.\\nLangGraph integrates seamlessly with\\nLangChain and\\nLangSmith (but does not require them).\\nTo learn more about LangGraph, check out our first LangChain Academy\\ncourse, Introduction to LangGraph, available for free\\nhere.\\nLangGraph Platform¬∂\\nLangGraph Platform is infrastructure for deploying LangGraph agents. It is a commercial solution for deploying agentic applications to production, built on the open-source LangGraph framework. The LangGraph Platform consists of several components that work together to support the development, deployment, debugging, and monitoring of LangGraph applications: LangGraph Server (APIs), LangGraph SDKs (clients for the APIs), LangGraph CLI (command line tool for building the server), and LangGraph Studio (UI/debugger).\\nSee deployment options here\\n(includes a free tier).\\nHere are some common issues that arise in complex deployments, which LangGraph Platform addresses:\\n\\nStreaming support: LangGraph Server provides multiple streaming modes optimized for various application needs\\nBackground runs: Runs agents asynchronously in the background\\nSupport for long running agents: Infrastructure that can handle long running processes\\nDouble texting: Handle the case where you get two messages from the user before the agent can respond\\nHandle burstiness: Task queue for ensuring requests are handled consistently without loss, even under heavy loads\\n\\nInstallation¬∂\\npip install -U langgraph\\n\\nExample¬∂\\nLet\\'s build a tool-calling ReAct-style agent that uses a search tool!\\npip install langchain-anthropic\\n\\nexport ANTHROPIC_API_KEY=sk-...\\n\\nOptionally, we can set up LangSmith for best-in-class observability.\\nexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=lsv2_sk_...\\n\\nThe simplest way to create a tool-calling agent in LangGraph is to use create_react_agent:\\n\\nHigh-level implementation\\nfrom langgraph.prebuilt import create_react_agent\\nfrom langgraph.checkpoint.memory import MemorySaver\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langchain_core.tools import tool\\n\\n# Define the tools for the agent to use\\n@tool\\ndef search(query: str):\\n    \"\"\"Call to surf the web.\"\"\"\\n    # This is a placeholder, but don\\'t tell the LLM that...\\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\\n        return \"It\\'s 60 degrees and foggy.\"\\n    return \"It\\'s 90 degrees and sunny.\"\\n\\n\\ntools = [search]\\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-latest\", temperature=0)\\n\\n# Initialize memory to persist state between graph runs\\ncheckpointer = MemorySaver()\\n\\napp = create_react_agent(model, tools, checkpointer=checkpointer)\\n\\n# Use the agent\\nfinal_state = app.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\\n    config={\"configurable\": {\"thread_id\": 42}}\\n)\\nfinal_state[\"messages\"][-1].content\\n\\n\"Based on the search results, I can tell you that the current weather in San Francisco is:\\\\n\\\\nTemperature: 60 degrees Fahrenheit\\\\nConditions: Foggy\\\\n\\\\nSan Francisco is known for its microclimates and frequent fog, especially during the summer months. The temperature of 60¬∞F (about 15.5¬∞C) is quite typical for the city, which tends to have mild temperatures year-round. The fog, often referred to as \"Karl the Fog\" by locals, is a characteristic feature of San Francisco\\\\\\'s weather, particularly in the mornings and evenings.\\\\n\\\\nIs there anything else you\\\\\\'d like to know about the weather in San Francisco or any other location?\"\\n\\n\\nNow when we pass the same \"thread_id\", the conversation context is retained via the saved state (i.e. stored list of messages)\\n\\nfinal_state = app.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what about ny\"}]},\\n    config={\"configurable\": {\"thread_id\": 42}}\\n)\\nfinal_state[\"messages\"][-1].content\\n\\n\"Based on the search results, I can tell you that the current weather in New York City is:\\\\n\\\\nTemperature: 90 degrees Fahrenheit (approximately 32.2 degrees Celsius)\\\\nConditions: Sunny\\\\n\\\\nThis weather is quite different from what we just saw in San Francisco. New York is experiencing much warmer temperatures right now. Here are a few points to note:\\\\n\\\\n1. The temperature of 90¬∞F is quite hot, typical of summer weather in New York City.\\\\n2. The sunny conditions suggest clear skies, which is great for outdoor activities but also means it might feel even hotter due to direct sunlight.\\\\n3. This kind of weather in New York often comes with high humidity, which can make it feel even warmer than the actual temperature suggests.\\\\n\\\\nIt\\'s interesting to see the stark contrast between San Francisco\\'s mild, foggy weather and New York\\'s hot, sunny conditions. This difference illustrates how varied weather can be across different parts of the United States, even on the same day.\\\\n\\\\nIs there anything else you\\'d like to know about the weather in New York or any other location?\"\\n\\n\\n\\nTip\\nLangGraph is a low-level framework that allows you to implement any custom agent\\narchitectures. Click on the low-level implementation below to see how to implement a\\ntool-calling agent from scratch.\\n\\n\\nLow-level implementation\\nfrom typing import Literal\\n\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langchain_core.tools import tool\\nfrom langgraph.checkpoint.memory import MemorySaver\\nfrom langgraph.graph import END, START, StateGraph, MessagesState\\nfrom langgraph.prebuilt import ToolNode\\n\\n\\n# Define the tools for the agent to use\\n@tool\\ndef search(query: str):\\n    \"\"\"Call to surf the web.\"\"\"\\n    # This is a placeholder, but don\\'t tell the LLM that...\\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\\n        return \"It\\'s 60 degrees and foggy.\"\\n    return \"It\\'s 90 degrees and sunny.\"\\n\\n\\ntools = [search]\\n\\ntool_node = ToolNode(tools)\\n\\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-latest\", temperature=0).bind_tools(tools)\\n\\n# Define the function that determines whether to continue or not\\ndef should_continue(state: MessagesState) -> Literal[\"tools\", END]:\\n    messages = state[\\'messages\\']\\n    last_message = messages[-1]\\n    # If the LLM makes a tool call, then we route to the \"tools\" node\\n    if last_message.tool_calls:\\n        return \"tools\"\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Define the function that calls the model\\ndef call_model(state: MessagesState):\\n    messages = state[\\'messages\\']\\n    response = model.invoke(messages)\\n    # We return a list, because this will get added to the existing list\\n    return {\"messages\": [response]}\\n\\n\\n# Define a new graph\\nworkflow = StateGraph(MessagesState)\\n\\n# Define the two nodes we will cycle between\\nworkflow.add_node(\"agent\", call_model)\\nworkflow.add_node(\"tools\", tool_node)\\n\\n# Set the entrypoint as `agent`\\n# This means that this node is the first one called\\nworkflow.add_edge(START, \"agent\")\\n\\n# We now add a conditional edge\\nworkflow.add_conditional_edges(\\n    # First, we define the start node. We use `agent`.\\n    # This means these are the edges taken after the `agent` node is called.\\n    \"agent\",\\n    # Next, we pass in the function that will determine which node is called next.\\n    should_continue,\\n)\\n\\n# We now add a normal edge from `tools` to `agent`.\\n# This means that after `tools` is called, `agent` node is called next.\\nworkflow.add_edge(\"tools\", \\'agent\\')\\n\\n# Initialize memory to persist state between graph runs\\ncheckpointer = MemorySaver()\\n\\n# Finally, we compile it!\\n# This compiles it into a LangChain Runnable,\\n# meaning you can use it as you would any other runnable.\\n# Note that we\\'re (optionally) passing the memory when compiling the graph\\napp = workflow.compile(checkpointer=checkpointer)\\n\\n# Use the agent\\nfinal_state = app.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\\n    config={\"configurable\": {\"thread_id\": 42}}\\n)\\nfinal_state[\"messages\"][-1].content\\n\\nStep-by-step Breakdown:\\n\\n\\nInitialize the model and tools.\\n\\n\\n    We use ChatAnthropic as our LLM. NOTE: we need to make sure the model knows that it has these tools available to call. We can do this by converting the LangChain tools into the format for OpenAI tool calling using the .bind_tools() method.\\n  \\n\\n    We define the tools we want to use - a search tool in our case. It is really easy to create your own tools - see documentation here on how to do that here.\\n  \\n\\n\\n\\nInitialize graph with state.\\n\\nWe initialize graph (StateGraph) by passing state schema (in our case MessagesState)\\nMessagesState is a prebuilt state schema that has one attribute -- a list of LangChain Message objects, as well as logic for merging the updates from each node into the state.\\n\\n\\n\\nDefine graph nodes.\\n\\nThere are two main nodes we need:\\n\\n\\nThe agent node: responsible for deciding what (if any) actions to take.\\nThe tools node that invokes tools: if the agent decides to take an action, this node will then execute that action.\\n\\n\\n\\nDefine entry point and graph edges.\\n\\nFirst, we need to set the entry point for graph execution - agent node.\\n\\nThen we define one normal and one conditional edge. Conditional edge means that the destination depends on the contents of the graph\\'s state (MessagesState). In our case, the destination is not known until the agent (LLM) decides.\\n\\n\\nConditional edge: after the agent is called, we should either:\\n    \\na. Run tools if the agent said to take an action, OR\\nb. Finish (respond to the user) if the agent did not ask to run tools\\n\\n\\nNormal edge: after the tools are invoked, the graph should always return to the agent to decide what to do next\\n\\n\\n\\nCompile the graph.\\n\\n\\n    When we compile the graph, we turn it into a LangChain \\n    Runnable, \\n    which automatically enables calling .invoke(), .stream() and .batch() \\n    with your inputs\\n  \\n\\n    We can also optionally pass checkpointer object for persisting state between graph runs, and enabling memory, \\n    human-in-the-loop workflows, time travel and more. In our case we use MemorySaver - \\n    a simple in-memory checkpointer\\n  \\n\\n\\n\\nExecute the graph.\\n\\nLangGraph adds the input message to the internal state, then passes the state to the entrypoint node, \"agent\".\\nThe \"agent\" node executes, invoking the chat model.\\nThe chat model returns an AIMessage. LangGraph adds this to the state.\\nGraph cycles the following steps until there are no more tool_calls on AIMessage:\\n    \\nIf AIMessage has tool_calls, \"tools\" node executes\\nThe \"agent\" node executes again and returns AIMessage\\n\\n\\nExecution progresses to the special END value and outputs the final state. And as a result, we get a list of all our chat messages as output.\\n\\n\\n\\nDocumentation¬∂\\n\\nTutorials: Learn to build with LangGraph through guided examples.\\nHow-to Guides: Accomplish specific things within LangGraph, from streaming, to adding memory & persistence, to common design patterns (branching, subgraphs, etc.), these are the place to go if you want to copy and run a specific code snippet.\\nConceptual Guides: In-depth explanations of the key concepts and principles behind LangGraph, such as nodes, edges, state and more.\\nAPI Reference: Review important classes and methods, simple examples of how to use the graph and checkpointing APIs, higher-level prebuilt components and more.\\nLangGraph Platform: LangGraph Platform is a commercial solution for deploying agentic applications in production, built on the open-source LangGraph framework.\\n\\nResources¬∂\\n\\nBuilt with LangGraph: Hear how industry leaders use LangGraph to ship powerful, production-ready AI applications.\\n\\nContributing¬∂\\nFor more information on how to contribute, see here.\\n\\n\\n\\n        Was this page helpful?\\n      \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Learn the basics\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright ¬© 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs Insiders\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCookie consent\\nWe use cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of our documentation and whether users find what they\\'re searching for. Clicking \"Accept\" makes our documentation better. Thank you! ‚ù§Ô∏è\\n\\n\\n\\n\\n\\n\\n\\n          Google Analytics\\n        \\n\\n\\n\\n\\n\\n          GitHub\\n        \\n\\n\\n\\n\\nAccept\\nReject\\n\\n\\n\\n\\n\\n\\n\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "docs = [WebBaseLoader(url).load()]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "print(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(max_results=2)\n",
    "# tools = [tool]\n",
    "# tool.invoke(\"What's a 'node' in LangGraph?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Prompt[rlm/rag-prompt]********************\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: \u001b[33;1m\u001b[1;3m{question}\u001b[0m \n",
      "Context: \u001b[33;1m\u001b[1;3m{context}\u001b[0m \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Generate Response Node\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm =  ChatGroq(temperature=0, model_name=\"gemma2-9b-it\", groq_api_key=GROQ_API_KEY)\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "print(\"*\" * 20 + \"Prompt[rlm/rag-prompt]\" + \"*\" * 20)\n",
    "prompt = hub.pull(\"rlm/rag-prompt\",api_key= LANGSMITH_API_KEY).pretty_print()  # Show what the prompt looks like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web search Agent\n",
    "\n",
    "def web_search_agent(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"messages\"]\n",
    "    query = question[0].content\n",
    "    print(question)\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": query})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    # web = Document(page_content=web_results)\n",
    "\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [web_results]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reframe the question\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n\n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n\n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question}\n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    llm =  ChatGroq(temperature=0, model_name=\"gemma2-9b-it\", groq_api_key=GROQ_API_KEY)\n",
    "    response = llm.invoke(msg)\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name = \"retrieve_blog_posts\",\n",
    "    description= \"Search and return information about langgraph\",\n",
    ")\n",
    "\n",
    "\n",
    "tools = [retriever_tool]\n",
    "\n",
    "# retrieval agent\n",
    "def retrieve_agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model =  ChatGroq(temperature=0, model_name=\"gemma2-9b-it\", groq_api_key=GROQ_API_KEY)\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document grader\n",
    "def grade_documents(state) -> Literal[\"generate_response\", \"web_search\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model =  ChatGroq(temperature=0, model_name=\"gemma2-9b-it\", groq_api_key=GROQ_API_KEY)\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate_response\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"web_search\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define graph ‚Äúworkflow‚Äù add nodes and edges/relationships and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict,Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START,END,StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "\n",
    "# Nodes\n",
    "workflow.add_node(\"web_search\",web_search_agent)\n",
    "workflow.add_node(\"retrieve\",retrieve_agent)\n",
    "\n",
    "retriever_node = ToolNode([retriever_tool])  # create retriever tool node\n",
    "\n",
    "workflow.add_node(\"retriever\", retriever_node)\n",
    "workflow.add_node(\"generate_response\", generate)\n",
    "\n",
    "# Edges\n",
    "workflow.add_edge(START,\"retrieve\")\n",
    "workflow.add_edge(\"web_search\",\"generate_response\")\n",
    "workflow.add_conditional_edges(\"retrieve\",\n",
    "                               tools_condition,\n",
    "                               {\"tools\":\"retriever\"},\n",
    "                               END)\n",
    "workflow.add_conditional_edges(\"retriever\",grade_documents)\n",
    "workflow.add_edge(\"generate_response\",END)\n",
    "\n",
    "# Compile workflow\n",
    "graph = workflow.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "\"Output from node 'retrieve':\"\n",
      "'---'\n",
      "{ 'messages': [ AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_p2f5', 'function': {'arguments': '{\"query\":\"how to play cricket\"}', 'name': 'retrieve_blog_posts'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 87, 'prompt_tokens': 961, 'total_tokens': 1048, 'completion_time': 0.158181818, 'prompt_time': 0.034005434, 'queue_time': 0.017939072, 'total_time': 0.192187252}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e14c506b-5b85-4984-8a5b-ed753cd997ad-0', tool_calls=[{'name': 'retrieve_blog_posts', 'args': {'query': 'how to play cricket'}, 'id': 'call_p2f5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 961, 'output_tokens': 87, 'total_tokens': 1048})]}\n",
      "'\\n---\\n'\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "\"Output from node 'retriever':\"\n",
      "'---'\n",
      "{ 'messages': [ ToolMessage(content='Thanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n                Next\\n\\nThanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n                Next\\n\\nThanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n                Next\\n\\nThanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n                Next', name='retrieve_blog_posts', id='3b25c870-9858-4a76-a6ab-ca6c27fd8bfd', tool_call_id='call_p2f5')]}\n",
      "'\\n---\\n'\n",
      "---WEB SEARCH---\n",
      "[HumanMessage(content='how do you play cricket?', additional_kwargs={}, response_metadata={}, id='96108a41-8c27-486d-bb8c-970de0b6dc62'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_p2f5', 'function': {'arguments': '{\"query\":\"how to play cricket\"}', 'name': 'retrieve_blog_posts'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 87, 'prompt_tokens': 961, 'total_tokens': 1048, 'completion_time': 0.158181818, 'prompt_time': 0.034005434, 'queue_time': 0.017939072, 'total_time': 0.192187252}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e14c506b-5b85-4984-8a5b-ed753cd997ad-0', tool_calls=[{'name': 'retrieve_blog_posts', 'args': {'query': 'how to play cricket'}, 'id': 'call_p2f5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 961, 'output_tokens': 87, 'total_tokens': 1048}), ToolMessage(content='Thanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n                Next\\n\\nThanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n                Next\\n\\nThanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n                Next\\n\\nThanks for your feedback!\\n            \\n\\n              \\n              \\n                \\n              \\n              Thanks for your feedback! Please help us improve this page by adding to the discussion below.\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n                Next', name='retrieve_blog_posts', id='3b25c870-9858-4a76-a6ab-ca6c27fd8bfd', tool_call_id='call_p2f5')]\n",
      "\"Output from node 'web_search':\"\n",
      "'---'\n",
      "{ 'messages': [ 'To play cricket, first gather 2 teams of 11 players. You will '\n",
      "                'also need a ball, a cricket bat, and two wickets. Wickets are '\n",
      "                'made of three sticks driven into the\\n'\n",
      "                'Cricket is played by two teams of 11, with one side taking a '\n",
      "                'turn to bat a ball and score runs, while the other team will '\n",
      "                'bowl and field the ball.']}\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalkothavade/Work/Data Science/GenAI/.lang_venv/lib/python3.11/site-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Output from node 'generate_response':\"\n",
      "'---'\n",
      "{ 'messages': [ 'Cricket is played with two teams of 11 players each. One team '\n",
      "                'bats, trying to score runs by hitting a ball with a bat, '\n",
      "                'while the other team bowls and fields the ball, aiming to get '\n",
      "                'the batsmen out.  The team with the most runs at the end of '\n",
      "                'the game wins. \\n']}\n",
      "'\\n---\\n'\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"how do you play cricket?\"),\n",
    "    ]\n",
    "}\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "\"Output from node 'retrieve':\"\n",
      "'---'\n",
      "{ 'messages': [ AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_5ref', 'function': {'arguments': '{\"query\":\"tools in langgraph\"}', 'name': 'retrieve_blog_posts'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 87, 'prompt_tokens': 961, 'total_tokens': 1048, 'completion_time': 0.158181818, 'prompt_time': 0.033900298, 'queue_time': 0.019200658999999995, 'total_time': 0.192082116}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-35b87934-e839-40ba-abc9-5ecb025ba129-0', tool_calls=[{'name': 'retrieve_blog_posts', 'args': {'query': 'tools in langgraph'}, 'id': 'call_5ref', 'type': 'tool_call'}], usage_metadata={'input_tokens': 961, 'output_tokens': 87, 'total_tokens': 1048})]}\n",
      "'\\n---\\n'\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "\"Output from node 'retriever':\"\n",
      "'---'\n",
      "{ 'messages': [ ToolMessage(content='Tip\\nLangGraph is a low-level framework that allows you to implement any custom agent\\narchitectures. Click on the low-level implementation below to see how to implement a\\ntool-calling agent from scratch.\\n\\n\\nLow-level implementation\\nfrom typing import Literal\\n\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langchain_core.tools import tool\\nfrom langgraph.checkpoint.memory import MemorySaver\\nfrom langgraph.graph import END, START, StateGraph, MessagesState\\nfrom langgraph.prebuilt import ToolNode\\n\\n\\n# Define the tools for the agent to use\\n@tool\\ndef search(query: str):\\n    \"\"\"Call to surf the web.\"\"\"\\n    # This is a placeholder, but don\\'t tell the LLM that...\\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\\n        return \"It\\'s 60 degrees and foggy.\"\\n    return \"It\\'s 90 degrees and sunny.\"\\n\\n\\ntools = [search]\\n\\ntool_node = ToolNode(tools)\\n\\nTip\\nLangGraph is a low-level framework that allows you to implement any custom agent\\narchitectures. Click on the low-level implementation below to see how to implement a\\ntool-calling agent from scratch.\\n\\n\\nLow-level implementation\\nfrom typing import Literal\\n\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langchain_core.tools import tool\\nfrom langgraph.checkpoint.memory import MemorySaver\\nfrom langgraph.graph import END, START, StateGraph, MessagesState\\nfrom langgraph.prebuilt import ToolNode\\n\\n\\n# Define the tools for the agent to use\\n@tool\\ndef search(query: str):\\n    \"\"\"Call to surf the web.\"\"\"\\n    # This is a placeholder, but don\\'t tell the LLM that...\\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\\n        return \"It\\'s 60 degrees and foggy.\"\\n    return \"It\\'s 90 degrees and sunny.\"\\n\\n\\ntools = [search]\\n\\ntool_node = ToolNode(tools)\\n\\nTip\\nLangGraph is a low-level framework that allows you to implement any custom agent\\narchitectures. Click on the low-level implementation below to see how to implement a\\ntool-calling agent from scratch.\\n\\n\\nLow-level implementation\\nfrom typing import Literal\\n\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langchain_core.tools import tool\\nfrom langgraph.checkpoint.memory import MemorySaver\\nfrom langgraph.graph import END, START, StateGraph, MessagesState\\nfrom langgraph.prebuilt import ToolNode\\n\\n\\n# Define the tools for the agent to use\\n@tool\\ndef search(query: str):\\n    \"\"\"Call to surf the web.\"\"\"\\n    # This is a placeholder, but don\\'t tell the LLM that...\\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\\n        return \"It\\'s 60 degrees and foggy.\"\\n    return \"It\\'s 90 degrees and sunny.\"\\n\\n\\ntools = [search]\\n\\ntool_node = ToolNode(tools)\\n\\nTip\\nLangGraph is a low-level framework that allows you to implement any custom agent\\narchitectures. Click on the low-level implementation below to see how to implement a\\ntool-calling agent from scratch.\\n\\n\\nLow-level implementation\\nfrom typing import Literal\\n\\nfrom langchain_anthropic import ChatAnthropic\\nfrom langchain_core.tools import tool\\nfrom langgraph.checkpoint.memory import MemorySaver\\nfrom langgraph.graph import END, START, StateGraph, MessagesState\\nfrom langgraph.prebuilt import ToolNode\\n\\n\\n# Define the tools for the agent to use\\n@tool\\ndef search(query: str):\\n    \"\"\"Call to surf the web.\"\"\"\\n    # This is a placeholder, but don\\'t tell the LLM that...\\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\\n        return \"It\\'s 60 degrees and foggy.\"\\n    return \"It\\'s 90 degrees and sunny.\"\\n\\n\\ntools = [search]\\n\\ntool_node = ToolNode(tools)', name='retrieve_blog_posts', id='d1fe15ca-a81f-43c1-8e36-bf7a44d42eef', tool_call_id='call_5ref')]}\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunalkothavade/Work/Data Science/GenAI/.lang_venv/lib/python3.11/site-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Output from node 'generate_response':\"\n",
      "'---'\n",
      "{ 'messages': [ 'LangGraph uses tools defined with the `@tool` decorator.  \\n'\n",
      "                'The provided example defines a tool called `search` that '\n",
      "                'returns weather information. \\n'\n",
      "                'Tools are then used within a `ToolNode` to enable the agent '\n",
      "                'to interact with them. \\n'\n",
      "                '\\n'\n",
      "                '\\n']}\n",
      "'\\n---\\n'\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"what are tools in langgraph\"),\n",
    "    ]\n",
    "}\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lang_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
